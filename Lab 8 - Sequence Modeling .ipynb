{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f647de7f",
   "metadata": {},
   "source": [
    "**In this assignment, you'll get to practice the concepts and skills covered in the course so far. The main objective of this assignment is to implement and use some of the tools, algorithms, and techniques to perform sentiment analysis on textual data..**\n",
    "\n",
    "\n",
    "\n",
    "**Guidelines**\n",
    "* Download `AmazonReviews.csv` file from D2L. \n",
    "* Make sure to run all the code cells, otherwise you may get errors like `NameError` for undefined variables.\n",
    "* Do not change variable names, delete cells or disturb other existing code. It may cause problems during evaluation.\n",
    "* In some cases, you may need to add some code cells or new statements before or after the line of code containing the `???`.\n",
    "* Use markdown cells to write your discussions and reflections. \n",
    "\n",
    "**Procedure**\n",
    "* Save your work as `IPYNB` file named `Lab8.ipynb` and submit to D2L `Lab 8 - Sequence Modeling (Dropbox)` by the due date.\n",
    "* As you go through this notebook, you will find the symbol `???` in certain places. To complete this assignment, you must replace all the `???` with appropriate values, expressions or statements to ensure that the notebook runs properly end-to-end.\n",
    "* Include your response for `Part 1` and `Part 2` in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e19a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Part 1: Activity \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee93f4",
   "metadata": {},
   "source": [
    "#### Read the content of the `AmazonReviews.csv` into a dataframe called `reviews_df` and perform the following:\n",
    "1. **Preprocess the `reviews_df` dataframe to prepare it for the following questions.**\n",
    "\n",
    "2. **Build a SimpleRNN Network to predict the sentiment of each review. Then, evaluate the performance of the model in terms of loss and accuracy** \n",
    "\n",
    "3. **Build a Gated Recurrent Unit to predict the sentiment of each review. Then, evaluate the performance of the model in terms of loss and accuracy** \n",
    "\n",
    "4. **Build a Long Short-Term Memory Model to predict the sentiment of each review. Then, evaluate the performance of the model in terms of loss and accuracy** \n",
    "\n",
    "5. **Which model performs better and why?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preprocess the reviews_df dataframe to prepare it for the following questions.\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a56c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import gzip\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib. cm as cm\n",
    "\n",
    "import nltk\n",
    "from nltk. sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import watermark\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb25c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/aishatolatunji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# used for natural language processing {NLTK: Natural Language Tool-Kit} \n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set a seed to reproduce the results later. Seed used here is '2019'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8abca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123456)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "#from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import watermark\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135eaa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(123456)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d5de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import watermark\n",
    "\n",
    "%load_ext watermark\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee00cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d95b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_df = pd.read_csv(\"AmazonReviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f29ad36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ebe074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3c998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_df.loc[Reviews_df['Score'] >= 3, 'sentiment'] = 1\n",
    "Reviews_df.loc[Reviews_df['Score'] < 3, 'sentiment'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58cfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "155c8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_df['Text'] = Reviews_df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1565c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_df['Text'] = Reviews_df['Text'].str.replace(r'[][]+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b45eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_df['Text'] = Reviews_df['Text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accb8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reviews_df['Text'] = Reviews_df['Text'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c389f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_df['Text']=Reviews_df['Text'].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57ec8ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_df['Text'] = Reviews_df['Text'].str.replace('\\d+', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07353206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba6a2007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_pos_embd import PositionEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "609f1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_pos_embd import TrigPosEmbedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd947ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e09ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Reviews_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "767cfe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28c56892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# The script below uses tfidf and then divides our data into 80% for the training set and 20% for the testing set\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500, ngram_range=(1,1))\n",
    "X_train = vectorizer.fit_transform(Reviews_df['Text'])[:20000].A \n",
    "Y_train = Reviews_df.sentiment[:20000] \n",
    "X_test = vectorizer.fit_transform(Reviews_df['Text'])[20000:].A\n",
    "Y_test = Reviews_df.sentiment[20000:]\n",
    "# The train set will be used to train our learning model while the testing set will be used to evalute hoe well our model performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef6efec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "X_train = keras.utils.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = keras.utils.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78534f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build a SimpleRNN Network to predict the sentiment of each review. Then, evaluate the performance of the model in terms of loss and accuracy\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f08e241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                2080      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 322,113\n",
      "Trainable params: 322,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 00:08:00.373792: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(keras.layers.Embedding(10000, 32, input_length=max_words))\n",
    "model.add(Embedding(10000, 32, input_length=max_words))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a20c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8bc9f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 9s 55ms/step - loss: 0.4501 - accuracy: 0.8486 - val_loss: 0.4259 - val_accuracy: 0.8486\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 8s 54ms/step - loss: 0.4180 - accuracy: 0.8528 - val_loss: 0.4256 - val_accuracy: 0.8486\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 8s 54ms/step - loss: 0.4180 - accuracy: 0.8528 - val_loss: 0.4251 - val_accuracy: 0.8486\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 8s 54ms/step - loss: 0.4180 - accuracy: 0.8528 - val_loss: 0.4253 - val_accuracy: 0.8486\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 9s 55ms/step - loss: 0.4180 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.4181 - accuracy: 0.8528 - val_loss: 0.4251 - val_accuracy: 0.8486\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.4179 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.4180 - accuracy: 0.8528 - val_loss: 0.4251 - val_accuracy: 0.8486\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.4182 - accuracy: 0.8528 - val_loss: 0.4257 - val_accuracy: 0.8486\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 8s 53ms/step - loss: 0.4182 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                   validation_data=(X_test, Y_test),\n",
    "                   epochs=10,\n",
    "                   batch_size=128,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c66714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build a Gated Recurrent Unit to predict the sentiment of each review. Then, evaluate the performance of the model in terms of loss and accuracy\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3bdd189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 32)                6336      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 326,369\n",
      "Trainable params: 326,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=max_words))\n",
    "model.add(GRU(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acdf6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5f6092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 20s 123ms/step - loss: 0.4437 - accuracy: 0.8528 - val_loss: 0.4269 - val_accuracy: 0.8486\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 19s 120ms/step - loss: 0.4184 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 19s 123ms/step - loss: 0.4184 - accuracy: 0.8528 - val_loss: 0.4257 - val_accuracy: 0.8486\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 19s 122ms/step - loss: 0.4181 - accuracy: 0.8528 - val_loss: 0.4251 - val_accuracy: 0.8486\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 19s 121ms/step - loss: 0.4183 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 20s 124ms/step - loss: 0.4186 - accuracy: 0.8528 - val_loss: 0.4278 - val_accuracy: 0.8486\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 19s 123ms/step - loss: 0.4182 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 19s 122ms/step - loss: 0.4183 - accuracy: 0.8528 - val_loss: 0.4254 - val_accuracy: 0.8486\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 19s 122ms/step - loss: 0.4185 - accuracy: 0.8528 - val_loss: 0.4251 - val_accuracy: 0.8486\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 19s 122ms/step - loss: 0.4182 - accuracy: 0.8528 - val_loss: 0.4254 - val_accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(X_train, Y_train,\n",
    "                   validation_data=(X_test, Y_test),\n",
    "                   epochs=10,\n",
    "                   batch_size=128,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa160359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build a Long Short-Term Memory Model to predict the sentiment of each review. Then, evaluate the performance of the model in terms of loss and accuracy\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3de80b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 328,353\n",
      "Trainable params: 328,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32, input_length=max_words))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ddf8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0607e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 21s 131ms/step - loss: 0.4448 - accuracy: 0.8528 - val_loss: 0.4256 - val_accuracy: 0.8486\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 20s 130ms/step - loss: 0.4183 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 20s 130ms/step - loss: 0.4181 - accuracy: 0.8528 - val_loss: 0.4253 - val_accuracy: 0.8486\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 20s 129ms/step - loss: 0.4183 - accuracy: 0.8528 - val_loss: 0.4263 - val_accuracy: 0.8486\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 20s 130ms/step - loss: 0.4183 - accuracy: 0.8528 - val_loss: 0.4264 - val_accuracy: 0.8486\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 20s 129ms/step - loss: 0.4181 - accuracy: 0.8528 - val_loss: 0.4256 - val_accuracy: 0.8486\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 20s 129ms/step - loss: 0.4183 - accuracy: 0.8528 - val_loss: 0.4254 - val_accuracy: 0.8486\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 20s 129ms/step - loss: 0.4181 - accuracy: 0.8528 - val_loss: 0.4252 - val_accuracy: 0.8486\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 20s 130ms/step - loss: 0.4182 - accuracy: 0.8528 - val_loss: 0.4261 - val_accuracy: 0.8486\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 20s 130ms/step - loss: 0.4183 - accuracy: 0.8528 - val_loss: 0.4251 - val_accuracy: 0.8486\n"
     ]
    }
   ],
   "source": [
    "history3 = model.fit(X_train, Y_train,\n",
    "                   validation_data=(X_test, Y_test),\n",
    "                   epochs=10,\n",
    "                   batch_size=128,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d06b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3992506",
   "metadata": {},
   "source": [
    "i. SimpleRnn network at the 10th iteration(i.e it used 10 epochs) produced an accuracy of 0.8528 and Val_accuracy of 0.8486\n",
    "\n",
    "ii. SimpleRnn network at the 10th iteration(i.e it used 10 epochs)  produced a loss  of 0.4182 and Val_loss of 0.4252\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a0a6e",
   "metadata": {},
   "source": [
    "i. Gated Recurrent unit at the 10th iteration(i.e it used 10 epochs) produced an accuracy of 0.8528 and Val_accuracy of 0.8486\n",
    "\n",
    "ii. Gated Recurrent unit at the 10th iteration(i.e it used 10 epochs)  produced a loss  of 0.4182 and Val_loss of 0.4254\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906a8dd",
   "metadata": {},
   "source": [
    "i. longshort term memmory model at the 10th iteration(i.e it used 10 epochs) produced an accuracy of 0.8528 and Val_accuracy of 0.8486\n",
    "\n",
    "ii. longshort term memmory model at the 10th iteration(i.e it used 10 epochs)  produced a loss  of 0.4183 and Val_loss of 0.4251\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99c2d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Which model performs better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42273262",
   "metadata": {},
   "source": [
    "The three models seems to provide the same accuracy and val accuracy but the longshort term memmory model seems to be the best model here because at the 10th iteration, it seems to minimize the val_loss at the rate of 0.4251 better the remaining models, so i would conclude that the long short term memmory model performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c4859",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Part 2: Reflection\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f41c86",
   "metadata": {},
   "source": [
    "As a second stepâ€”after answering the questions, include the following:\n",
    "1. A reflection of your experience performing the activity. \n",
    "2. A reflection on the importance of learning this activity.\n",
    "**Note:** include your reflection in this notebook as markdown cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91f7d8",
   "metadata": {},
   "source": [
    "A reflection of your experience performing the activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493e6ddb",
   "metadata": {},
   "source": [
    "Working on this particular project drove me back to machine learning on how to split data set using validation approach and it's important to avoid over fitting.\n",
    "\n",
    "i. i also understood that the two losses(both loss and val_loss) are decreasing and the accuracy (acc and val_acc) are increasing which indicates the modelling is trained in a good way.\n",
    "\n",
    "ii. The val_acc is the measure of how good the predictions of your model are. in this case the project used 10 epochs i.e it iterated 10 times, this study also explains to me we do not need to increase the iteration times as it would still produce the same result.\n",
    "iv. i also noticed that the accuaracy and the val accuracy remained static which means it's good at 10th iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522eb1f1",
   "metadata": {},
   "source": [
    "A reflection on the importance of learning this activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286afc7c",
   "metadata": {},
   "source": [
    "In deep learning, the loss is the value that a neural network is trying to minimize: it's the distance between the ground truth and the predictions. In order to minimize this distance, the neural network learns by adjusting weights and biases in a manner that reduces the loss.\n",
    "\n",
    "In classification, it's a little more complicated, but very similar. Predicted classes are based on probability. The loss is therefore also based on probability. In classification, the neural network minimizes the likelihood to assign a low probability to the actual class. The loss is typically categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9168b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "844c3dbe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Submission\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd22e1a",
   "metadata": {},
   "source": [
    "Submit **Lab8.ipynb** to the **Lab 8 - Sequence Modeling (Dropbox)** on D2L by the due date. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053c344",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Grading Rubric\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c85704",
   "metadata": {},
   "source": [
    "|Criterion\t|Excellent\t|Good\t|Average\t|Below Average\t|Poor\t|No Attempt|\n",
    "|:--\t|:--\t|:--\t|:--\t|:-- \t|:--\t|:-- |\n",
    "|**Part 1:** Activity-Question 1|\t**10 points**- Completes all aspects of the question correctly\t|**8 points**- Completes most aspects of the question correctly |**6 points**- Completes aspects of the question  correctly, and some incorrectly\t|**4 points**- Completes most aspects of the question  incorrectly or does not attempt many aspects|\t**2 points**- Minimal effort or completes a few aspects of the question or very few correctly|**0 points**- Did not complete the question|\n",
    "|**Part 1:** Activity-Question 2|\t**10 points**- Completes all aspects of the question correctly\t|**8 points**- Completes most aspects of the question correctly |**6 points**- Completes aspects of the question  correctly, and some incorrectly\t|**4 points**- Completes most aspects of the question  incorrectly or does not attempt many aspects|\t**2 points**- Minimal effort or completes a few aspects of the question or very few correctly|**0 points**- Did not complete the question|\n",
    "|**Part 1:** Activity-Question 3|\t**10 points**- Completes all aspects of the question correctly\t|**8 points**- Completes most aspects of the question correctly |**6 points**- Completes aspects of the question  correctly, and some incorrectly\t|**4 points**- Completes most aspects of the question  incorrectly or does not attempt many aspects|\t**2 points**- Minimal effort or completes a few aspects of the question or very few correctly|**0 points**- Did not complete the question|\n",
    "|**Part 1:** Activity-Question 4|\t**10 points**- Completes all aspects of the question correctly\t|**8 points**- Completes most aspects of the question correctly |**6 points**- Completes aspects of the question  correctly, and some incorrectly\t|**4 points**- Completes most aspects of the question  incorrectly or does not attempt many aspects|\t**2 points**- Minimal effort or completes a few aspects of the question or very few correctly|**0 points**- Did not complete the question|\n",
    "|**Part 1:** Activity-Question 5|\t**10 points**- Completes all aspects of the question correctly\t|**8 points**- Completes most aspects of the question correctly |**6 points**- Completes aspects of the question  correctly, and some incorrectly\t|**4 points**- Completes most aspects of the question  incorrectly or does not attempt many aspects|\t**2 points**- Minimal effort or completes a few aspects of the question or very few correctly|**0 points**- Did not complete the question|\n",
    "|**Part 2:** Reflection|**10 points**- Reflection clearly ties to the module content; experience and importance clearly laid out|**8 points**- Reflection mostly ties to the module content; experience & importance are discussed|**6 points**- Reflection ties minimally to the module content; experience & importance are discussed but not thoroughly|**4 points**- Reflection does not tie to the module content; experience & importance are minimally discussed|**2 points**- Minimal effort to tie to content; minimal effort to describe experience/ importance|**0 points**- Did not complete the reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26913da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.052px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
